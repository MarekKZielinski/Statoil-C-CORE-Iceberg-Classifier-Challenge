{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff7a86ca0b5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import heapq\n",
    "import xgboost as xgb\n",
    "import h5py\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Reshape, Lambda, ZeroPadding2D, GaussianNoise, AlphaDropout, Input, Concatenate\n",
    "from keras.layers.core import Flatten, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.utils import to_categorical, normalize\n",
    "from keras.models import model_from_json\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import ndimage\n",
    "from skimage.morphology import reconstruction\n",
    "from skimage.restoration import denoise_wavelet, denoise_tv_chambolle, denoise_nl_means\n",
    "\n",
    "from clr_callback import CyclicLR\n",
    "\n",
    "random_seed = 54321\n",
    "np.random.seed(random_seed)\n",
    "cwd = os.getcwd()\n",
    "#for windows\n",
    "model_path = cwd + '\\\\models\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually create tensorflow session to avoid potential OEM errors on laptop's GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(random_seed)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\"Data/train/train.json\", orient='records')\n",
    "data.head()\n",
    "train_df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['inc_angle_f'] = pd.to_numeric(train_df['inc_angle'], errors='coerce')\n",
    "print(\"missing values in inc_angle: \", train_df['inc_angle_f'].isnull().sum())\n",
    "#train_df['inc_angle_f'].replace(np.nan,train_df['inc_angle_f'].mean(), inplace=True)\n",
    "train_df['inc_angle_f'].replace(np.nan,0, inplace=True)\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bands(train_df):\n",
    "    max_col = np.array(train_df.apply(lambda x: max((max(train_df.loc[x.name,'band_1']),max(train_df.loc[x.name,'band_2']))),axis=1)) - 10\n",
    "    max_col2 = max_col.reshape(-1,1) * np.ones(75*75).reshape(1,75*75)\n",
    "    max_col2 = max_col2.reshape(-1,75,75)\n",
    "\n",
    "    band_1 = np.array(train_df['band_1'].tolist()).reshape(-1,75,75) - max_col2\n",
    "    band_2 = np.array(train_df['band_2'].tolist()).reshape(-1,75,75) - max_col2\n",
    "    band_1_t = 10**(band_1/10)\n",
    "    band_2_t = 10**(band_2/10)\n",
    "    band_1_t = np.where(band_1_t > 0.01, band_1_t, 0)\n",
    "    band_2_t = np.where(band_2_t > 0.01, band_2_t, 0)\n",
    "    band_3 = band_1_t - band_2_t\n",
    "    X = np.stack((band_1,band_2,band_1_t,band_2_t),axis=3)\n",
    "    \n",
    "    return band_1, band_2, band_1_t, band_2_t, band_3, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band_1, band_2, band_1_t, band_2_t, band_3, X = get_bands(train_df)\n",
    "plt.hist(band_1.flatten(), bins=200, color=\"red\", alpha=0.4)\n",
    "plt.hist(band_2.flatten(), bins=200, color=\"blue\", alpha=0.4)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(band_1[train_df[train_df['is_iceberg']==0].index[:3]].flatten(), bins=50, color=\"orange\", alpha=0.4)\n",
    "plt.hist(band_1[train_df[train_df['is_iceberg']==1].index[:3]].flatten(), bins=50, color=\"green\", alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(band_1_t.flatten(),bins=200, color=\"red\", alpha=0.4)\n",
    "plt.hist(band_2_t.flatten(),bins=200, color=\"blue\", alpha=0.4)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(band_3[train_df[train_df['is_iceberg']==0].index].flatten(), bins=50, color=\"orange\", alpha=0.4)\n",
    "plt.hist(band_3[train_df[train_df['is_iceberg']==1].index].flatten(), bins=50, color=\"green\", alpha=0.4)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_bands(index, cmap=\"gray\"):\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    fig.suptitle(\"Is Iceberg: %x\" % (train_df.loc[index,'is_iceberg']), fontsize=16)\n",
    "    ax1 = fig.add_subplot(251)\n",
    "    ax1.set_title(\"Band 1\")\n",
    "    ax1.imshow(band_1[index], cmap=cmap)\n",
    "    ax2 = fig.add_subplot(252)\n",
    "    ax2.set_title(\"Band 2\")\n",
    "    ax2.imshow(band_2[index], cmap=cmap)\n",
    "    ax3 = fig.add_subplot(253)\n",
    "    ax3.set_title(\"Band 1 t\")\n",
    "    ax3.imshow(band_1_t[index], cmap=cmap)\n",
    "    ax3 = fig.add_subplot(254)\n",
    "    ax3.set_title(\"Band 2 t\")\n",
    "    ax3.imshow(band_2_t[index], cmap=cmap)\n",
    "    ax3 = fig.add_subplot(255)\n",
    "    ax3.set_title(\"Band 3\")\n",
    "    ax3.imshow(band_3[index], cmap=cmap)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "plot_bands(0,cmap=\"inferno\")\n",
    "plot_bands(2,cmap=\"inferno\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = train_df.loc[:,'is_iceberg']\n",
    "y_angle = train_df.loc[:,['is_iceberg','inc_angle_f']]\n",
    "y_angle['index'] = y_angle.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_angle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y_angle, test_size=0.3, random_state=random_seed)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_sample = X_train[:]\n",
    "y_train_sample = y_train[:]\n",
    "print(X_train_sample.shape)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "datagen_val = ImageDataGenerator(\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    rotation_range=0,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "#custom generator for fit_generator\n",
    "from collections import Generator\n",
    "class Datagen_angle(Generator):\n",
    "    def __init__(self, imagegen=ImageDataGenerator):\n",
    "        self.imagegen = imagegen\n",
    "        \n",
    "    def flow(self, x, y, batch_size=8, shuffle=True):\n",
    "        self.generator = self.imagegen.flow(x, y, batch_size=batch_size, shuffle=shuffle)\n",
    "        return self\n",
    "    \n",
    "    def send(self, ignored):\n",
    "        temp_data = next(self.generator)\n",
    "        temp_band_3 = temp_data[0][:,:,:,2] - temp_data[0][:,:,:,3] #band_1_t - band_2_t\n",
    "        temp_stacked1 = np.stack((temp_data[0][:,:,:,0],temp_data[0][:,:,:,1]),axis=3)\n",
    "        temp_stacked2 = np.stack((temp_data[0][:,:,:,2],temp_data[0][:,:,:,3],temp_band_3),axis=3)\n",
    "        nn_denoised_temp = temp_data[0] #pass 4 bands for nn denoising input\n",
    "        return [temp_stacked1, temp_stacked2, \n",
    "                nn_denoised_temp,\n",
    "                temp_data[1][:,1]], temp_data[1][:,0]\n",
    "    \n",
    "    def throw(self, type=None, value=None, traceback=None):\n",
    "        raise StopIteration\n",
    "    \n",
    "\n",
    "datagen.fit(X_train_sample)\n",
    "datagen_val.fit(X_val)\n",
    "\n",
    "datagen_angle = Datagen_angle(imagegen=datagen)\n",
    "datagen_angle_val = Datagen_angle(imagegen=datagen_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduler and callback definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "class LScheduler:\n",
    "    def __init__(self, initial_lrate=0.001, drop=0.66, patience=5):\n",
    "        self.initial_lrate=initial_lrate\n",
    "        self.drop = drop\n",
    "        self.patience = patience\n",
    "\n",
    "    def step_decay(self,epoch):\n",
    "        initial_lrate = self.initial_lrate\n",
    "        drop = self.drop\n",
    "        patience = self.patience\n",
    "        lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/patience))\n",
    "        if math.fmod(epoch, patience) == 0:\n",
    "            print(\"Setting learning rate: \",lrate)\n",
    "        return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def denoising(img):\n",
    "    img_list = []\n",
    "    for i in range(4):\n",
    "        image = normalize(img[:,:,i])\n",
    "        img_list.append(ndimage.median_filter(image, 3))\n",
    "    return np.stack(img_list,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_over_axis(func, data, mask=None, axis=0, *args, **kwargs):\n",
    "    f_list = []\n",
    "    for i in range(data.shape[axis]):\n",
    "        if mask is None:\n",
    "            f_list.append(func(data[i], *args, **kwargs))\n",
    "        else:\n",
    "            f_list.append(func(data[i], mask=mask[i], *args, **kwargs))\n",
    "    return np.stack(f_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_denoised = apply_over_axis(denoising, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#index=8\n",
    "#original_index = y_train_sample.iloc[index].name\n",
    "#cmap=\"inferno\"\n",
    "#fig = plt.figure(figsize=(12,6))\n",
    "#fig.suptitle(\"Denoising: is iceberg: %x\" % (y_train_sample.iloc[index,0]), fontsize=16)\n",
    "#ax1 = fig.add_subplot(251)\n",
    "#ax1.set_title(\"Before\")\n",
    "#ax1.imshow(X_train_sample[index][:,:,0], cmap=cmap)\n",
    "#ax2 = fig.add_subplot(252)\n",
    "#ax2.set_title(\"Denoised\")\n",
    "#ax2.imshow(X_denoised[original_index][:,:,0], cmap=cmap)\n",
    "#ax1 = fig.add_subplot(253)\n",
    "#ax1.set_title(\"Before - band 2\")\n",
    "#ax1.imshow(X_train_sample[index][:,:,1], cmap=cmap)\n",
    "#ax2 = fig.add_subplot(254)\n",
    "#ax2.set_title(\"Denoised - band 2\")\n",
    "#ax2.imshow(X_denoised[original_index][:,:,1], cmap=cmap)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#custom generator for denoising\n",
    "from collections import Generator\n",
    "class Datagen_denoising(Generator):\n",
    "    def __init__(self, imagegen=ImageDataGenerator):\n",
    "        self.imagegen = imagegen\n",
    "        \n",
    "    def flow(self, x, y, batch_size=8, shuffle=True):\n",
    "        self.generator = self.imagegen.flow(x, y, batch_size=batch_size, shuffle=shuffle)\n",
    "        return self\n",
    "    \n",
    "    def send(self, ignored):\n",
    "        temp_data = next(self.generator)\n",
    "        temp_stacked1 = np.stack((temp_data[0][:,:,:,0],temp_data[0][:,:,:,1]),axis=3)\n",
    "        temp_stacked = np.stack((temp_data[0][:,:,:,0],temp_data[0][:,:,:,1],temp_data[0][:,:,:,2],\n",
    "                                temp_data[0][:,:,:,3]),axis=3)\n",
    "        return temp_stacked, temp_stacked\n",
    "    \n",
    "    def throw(self, type=None, value=None, traceback=None):\n",
    "        raise StopIteration\n",
    "        \n",
    "datagen_denoising = Datagen_denoising(imagegen=datagen)\n",
    "datagen_denoising_val = Datagen_denoising(imagegen=datagen_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_input = Input(shape=(75,75,4), name='m_input')\n",
    "\n",
    "#conv layers for main_input\n",
    "x1 = BatchNormalization()(m_input)\n",
    "x1 = ZeroPadding2D()(x1)\n",
    "x1 = Conv2D(8, (3,3), activation='relu')(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.2)(x1)\n",
    "x1 = ZeroPadding2D()(x1)\n",
    "x1 = Conv2D(8, (3,3), activation='relu')(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.2)(x1)\n",
    "x1 = ZeroPadding2D()(x1)\n",
    "m_output = Conv2D(4, (3,3), activation='linear', name='m_output')(x1)\n",
    "model_denoise = Model(inputs=[m_input,], outputs=[m_output], name='Model_nn_denoising')\n",
    "\n",
    "model_denoise.compile(optimizer=Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0),\n",
    "loss='mean_squared_error',\n",
    "metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_denoise.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model training\n",
    "#lScheduler_denoising = LScheduler(initial_lrate=0.1, drop=0.66, patience=3)\n",
    "#lrScheduler_denoising = LearningRateScheduler(lScheduler_denoising.step_decay)\n",
    "lrScheduler_denoising = CyclicLR(base_lr=1e-8, max_lr=0.006,\n",
    "                                 step_size=400, mode='triangular2', gamma=0.99994)\n",
    "start_time = time.monotonic()\n",
    "\n",
    "H = model_denoise.fit_generator(datagen_denoising.flow(X, y_angle, batch_size=8),\n",
    "                    steps_per_epoch=len(X)/8,\n",
    "                    validation_data=datagen_denoising_val.flow(X, y_angle, batch_size=8, shuffle=False), \n",
    "                    validation_steps=len(X)/8,\n",
    "                    #validation_data=[X_val,y_val],\n",
    "                    epochs=12,\n",
    "                    callbacks = [lrScheduler_denoising, \n",
    "                                 TQDMNotebookCallback(leave_inner=True, leave_outer=True)],\n",
    "                    verbose=0)\n",
    "\n",
    "model_time = time.monotonic() - start_time\n",
    "print(\"Model training time: \" + '{:d}'.format(int(model_time // 60)) + \" minutes \" \n",
    "      + '{:.1f}'.format(model_time % 60) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = lrScheduler_denoising.history\n",
    "plt.plot(h['lr'], color=\"b\", label='lr')\n",
    "plt.legend()\n",
    "plt.xlabel('# iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model_denoise.to_json()\n",
    "with open(\"models/model_denoise.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model_weights = model_denoise.get_weights()\n",
    "with open('models/model_denoise_weights.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_weights, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "with open(\"models/model_denoise.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model_denoise = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "with open('models/model_denoise_weights.pickle', 'rb') as handle:\n",
    "    model_weights = pickle.load(handle)\n",
    "model_denoise.set_weights(model_weights)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_nn_denoised = model_denoise.predict(X, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=8\n",
    "original_index = y_train_sample.iloc[index].name\n",
    "cmap=\"inferno\"\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle(\"Image denoising nn: %x\" % (train_df.loc[original_index,'is_iceberg']), fontsize=16)\n",
    "ax1 = fig.add_subplot(251)\n",
    "ax1.set_title(\"Before band_1\")\n",
    "ax1.imshow(X_train_sample[index][:,:,0], cmap=cmap)\n",
    "ax2 = fig.add_subplot(252)\n",
    "ax2.set_title(\"NN Denoising band 1\")\n",
    "ax2.imshow(X_nn_denoised[original_index][:,:,0], cmap=cmap)\n",
    "ax3 = fig.add_subplot(253)\n",
    "ax3.set_title(\"Before band 2\")\n",
    "ax3.imshow(X_train_sample[index][:,:,1], cmap=cmap)\n",
    "ax4 = fig.add_subplot(254)\n",
    "ax4.set_title(\"NN Denoising band 2\")\n",
    "ax4.imshow(X_nn_denoised[original_index][:,:,1], cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_code=\"CNN_2018_01_21_v01\"\n",
    "model_comment=\"2 CNN inputs 3,3 conv filters - 3rd input nn denoising, na=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%writefile current_model.py\n",
    "\n",
    "def InputBlock(x, dropout=0.2, prefix=''):\n",
    "    #conv layers for input\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x) \n",
    "    return(x)\n",
    "\n",
    "main_input = Input(shape=(75,75,2), name='main_input')\n",
    "aux_input = Input(shape=(75,75,3), name='aux_input')\n",
    "aux_input_nn = Input(shape=(75,75,4), name='aux_input_nn')\n",
    "\n",
    "x1 = InputBlock(main_input, prefix='m_input')\n",
    "x2 = InputBlock(aux_input, prefix='a_input')\n",
    "x3 = model_denoise(aux_input_nn)\n",
    "x3 = InputBlock(x3,dropout=0.2, prefix='a_input_nn')\n",
    "\n",
    "x = Concatenate(axis=3)([x1,x2,x3])\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "\n",
    "#conv-block\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#conv-block\n",
    "x = Conv2D(256, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "    \n",
    "#flatten\n",
    "x = Flatten()(x)\n",
    "angle_input = Input(shape=[1], name='angle_input')\n",
    "#x1 = BatchNormalization()(angle_input)\n",
    "merged = Concatenate()([x, angle_input])\n",
    "\n",
    "#dense-block\n",
    "x = Dense(513, activation='relu')(merged)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#dense-block\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
    "model_f = Model(inputs=[main_input,aux_input, \n",
    "                        aux_input_nn, \n",
    "                        angle_input], \n",
    "                        outputs=[main_output])\n",
    "\n",
    "model_f.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0),\n",
    "loss='binary_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i current_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelHistory(Callback):\n",
    "    def __init__(self, listSize=10):\n",
    "        self.listSize = listSize\n",
    "        self.models = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        lastLoss = logs.get('val_loss')\n",
    "        rank = 1 - lastLoss\n",
    "        if len(self.models) > 0:\n",
    "            if rank > self.models[0][0]: # new model is better than the worst in the heap\n",
    "                if len(self.models) >= self.listSize: #if the model heap is already full\n",
    "                    heapq.heappushpop(self.models, (rank, lastLoss, self.model.get_weights()))\n",
    "                else:\n",
    "                    heapq.heappush(self.models, (rank, lastLoss, self.model.get_weights()))\n",
    "        else:\n",
    "            heapq.heappush(self.models, (rank, lastLoss, self.model.get_weights()))\n",
    "\n",
    "def get_callbacks(filepath, save_to_disc = True, lScheduler = None,\n",
    "                  patience=10, step_decay=LScheduler().step_decay, modelHistoryCallback=None):\n",
    "    #es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    #reduceLr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "    #                          patience=5, min_lr=0.000001, verbose=1)\n",
    "    if lScheduler is None:\n",
    "        lrScheduler = LearningRateScheduler(step_decay)\n",
    "    else:\n",
    "        lrScheduler = lScheduler\n",
    "    tqdmCallback =  TQDMNotebookCallback(leave_inner=True, leave_outer=True)\n",
    "    if (save_to_disc):\n",
    "        return [msave, lrScheduler, modelHistoryCallback, tqdmCallback]\n",
    "    else:\n",
    "        return [lrScheduler, modelHistoryCallback, tqdmCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_f.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "#import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'd:/Anaconda3/Library/bin/graphviz/'\n",
    "SVG(model_to_dot(model_f).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#name init\n",
    "model_timestamp = str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
    "model_best_weights_path = model_path + \"weights.\" + model_code + \"_\" + model_timestamp + \".hdf5\"\n",
    "#lScheduler = LScheduler(initial_lrate=0.001, drop=0.66, patience=7)\n",
    "modelEnsemble = ModelHistory(listSize=21)\n",
    "lScheduler = CyclicLR(base_lr=1e-9, max_lr=0.001,\n",
    "                                 step_size=300, mode='traingular3', beta=0.66)\n",
    "callbacks = get_callbacks(filepath=model_best_weights_path, save_to_disc=False, lScheduler=lScheduler,\n",
    "                          modelHistoryCallback=modelEnsemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model training\n",
    "start_time = time.monotonic()\n",
    "\n",
    "H = model_f.fit_generator(datagen_angle.flow(X_train_sample, y_train_sample, batch_size=16),\n",
    "                    steps_per_epoch=len(X_train_sample)/16,\n",
    "                    validation_data=datagen_angle_val.flow(X_val, y_val, batch_size=24, shuffle=False), \n",
    "                    validation_steps=len(X_val)/24,\n",
    "                    #validation_data=[X_val,y_val],\n",
    "                    epochs=70, callbacks=callbacks,\n",
    "                    verbose=0)\n",
    "\n",
    "model_time = time.monotonic() - start_time\n",
    "print(\"Model training time: \" + '{:d}'.format(int(model_time // 60)) + \" minutes \" \n",
    "      + '{:.1f}'.format(model_time % 60) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = lScheduler.history\n",
    "plt.plot(h['lr'], color=\"b\", label='lr')\n",
    "plt.legend()\n",
    "plt.xlabel('# iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(H.history['loss'], color=\"b\", label='Training loss')\n",
    "plt.plot(H.history['val_loss'], color=\"r\", label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('# epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_f.set_weights(modelEnsemble.models[len(modelEnsemble.models)-1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional training epochs with SGD - warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#addtional training epochs - warm start\n",
    "#lScheduler = LScheduler(initial_lrate=0.000001, drop=0.66, patience=3)\n",
    "modelEnsemble2 = ModelHistory(listSize=5)\n",
    "lScheduler = CyclicLR(base_lr=1e-9, max_lr=1e-5,\n",
    "                                 step_size=80, mode='triangular2', gamma=0.9994)\n",
    "callbacks = get_callbacks(filepath=model_best_weights_path, save_to_disc=False, lScheduler=lScheduler,\n",
    "                          modelHistoryCallback=modelEnsemble2)\n",
    "model_f.compile(optimizer=SGD(lr=0.0001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "start_time = time.monotonic()\n",
    "\n",
    "H2 = model_f.fit_generator(datagen_angle.flow(X_train_sample, y_train_sample, batch_size=24, shuffle=False),\n",
    "                    steps_per_epoch=len(X_train_sample)/24,\n",
    "                    validation_data=datagen_angle_val.flow(X_val, y_val, batch_size=24, shuffle=False), \n",
    "                    validation_steps=len(X_val)/24,\n",
    "                    #validation_data=[X_val,y_val],\n",
    "                    epochs=15, callbacks=callbacks,\n",
    "                    verbose=0)\n",
    "\n",
    "model_time = time.monotonic() - start_time\n",
    "print(\"Model training time: \" + '{:d}'.format(int(model_time // 60)) + \" minutes \" \n",
    "      + '{:.1f}'.format(model_time % 60) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = lScheduler.history\n",
    "plt.plot(h['lr'], color=\"b\", label='lr')\n",
    "plt.legend()\n",
    "plt.xlabel('# iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in H.history:\n",
    "    H.history[key].extend(H2.history[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(H2.history['loss'], color=\"b\", label='Training loss')\n",
    "plt.plot(H2.history['val_loss'], color=\"r\", label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('# epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(H.history['loss'], color=\"b\", label='Training loss')\n",
    "plt.plot(H.history['val_loss'], color=\"r\", label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('# epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model_f.to_json()\n",
    "with open(\"models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model from JSON - don't care about the weights rith now, they are saved separately\n",
    "with open(\"models/model.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "    model_f = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model_object_path = model_path + \"model.\" + model_code + \"_\" + model_timestamp + '.hdf5'\n",
    "#model_f.save('models/last_model.hdf5') //crashes python kernel with Keras version 2.1.2\n",
    "#model_f = load_model(model_object_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "argmin = np.array(H.history[\"loss\"]).argmin()\n",
    "argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "argmin = np.array(H.history[\"val_loss\"]).argmin()\n",
    "argmax_acc = np.array(H.history[\"val_acc\"]).argmax()\n",
    "#with open('current_model.py','r') as model_python_code_file:\n",
    "#    models_history = pd.DataFrame({\"timestamp\":[model_timestamp], \n",
    "#                                    \"val_loss [min]\":[H.history['val_loss'][argmin]],\n",
    "#                                    \"epoch [val_loss [min]]\":argmin,\n",
    "#                                    \"training_loss [val_loss [min]]\":[H.history['loss'][argmin]],\n",
    "#                                    \"val_acc [val_loss [min]]\":[H.history['val_acc'][argmin]],\n",
    "#                                    \"training_acc [val_loss [min]]\":[H.history['acc'][argmin]],\n",
    "#                                    \n",
    "#                                    \"val_acc [max]\":[H.history['val_acc'][argmax_acc]],\n",
    "#                                    \"epoch [val_acc [max]]\":argmax_acc,\n",
    "#                                    \"training_loss [val_acc [max]]\":[H.history['loss'][argmax_acc]],\n",
    "#                                    \"val_loss [val_acc [max]]\":[H.history['val_loss'][argmax_acc]],\n",
    "#                                    \"training_acc [val_acc [max]]\":[H.history['acc'][argmax_acc]],\n",
    "#                                    \n",
    "#                                    \"model_path\":[model_object_path],\n",
    "#                                    \"model_weights_path\":[model_best_weights_path],\n",
    "#                                    \"model_python_code\":[model_python_code_file.read().replace('\\r\\n','\\n')],\n",
    "#                                    \"model_comment\":[model_comment]\n",
    "#                                })\n",
    "#                               \n",
    "#models_history = models_history[[\"timestamp\", \n",
    "#                                 \"epoch [val_loss [min]]\", \"val_loss [min]\", \"training_loss [val_loss [min]]\",\n",
    "#                                 \"val_acc [val_loss [min]]\", \"training_acc [val_loss [min]]\",\n",
    "#                                 \"epoch [val_acc [max]]\", \"val_acc [max]\", \"training_loss [val_acc [max]]\",\n",
    "#                                 \"val_loss [val_acc [max]]\", \"training_acc [val_acc [max]]\",\n",
    "#                                 \"model_path\",\"model_weights_path\",\"model_python_code\",\"model_comment\"]]\n",
    "#models_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"Min validation loss epoch:\")\n",
    "#print(\"epoch: %d\" %(argmin),\n",
    "#      \"; val loss [min] %.4f: \" % (models_history[\"val_loss [min]\"][0]), \n",
    "#      \"; training loss: %.4f\" % (models_history[\"training_loss [val_loss [min]]\"][0]),\n",
    "#      \"; val acc: %.4f\" % (models_history[\"val_acc [val_loss [min]]\"][0]),\n",
    "#      \"; training acc: %.4f \" % (models_history[\"training_acc [val_loss [min]]\"][0])\n",
    "#     )\n",
    "#print(\"Max validation accuracy epoch:\")\n",
    "#print(\"epoch: %d\" %(argmax_acc),\n",
    "#      \"; val loss %.4f: \" % (models_history[\"val_loss [val_acc [max]]\"][0]), \n",
    "#      \"; training loss: %.4f\" % (models_history[\"training_loss [val_acc [max]]\"][0]),\n",
    "#      \"; val acc [max]: %.4f\" % (models_history[\"val_acc [max]\"][0]),\n",
    "#      \"; training acc: %.4f \" % (models_history[\"training_acc [val_acc [max]]\"][0]),\n",
    "#     )\n",
    "#print(\"model comment:\", model_comment)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('models_history.csv', 'a') as f:\n",
    "#    models_history.to_csv(f, header=False,index=False)\n",
    "#    #models_history.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('models_history.csv')\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelEnsemble.models.extend(modelEnsemble2.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('models/modelEnsemble.pickle', 'wb') as handle:\n",
    "    pickle.dump(modelEnsemble.models, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('models/modelEnsemble.pickle', 'rb') as handle:\n",
    "    modelEnsemble.models = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(model,weights, X, y):\n",
    "    model.set_weights(weights)\n",
    "    return model.predict_generator(datagen_angle_val.flow(X, y, batch_size=32, shuffle=False), \n",
    "                           steps = len(X)/31, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ensemble_predictions(X, y, modelEnsemble):\n",
    "    predictions = [get_prediction(model_f, model[2], X, y)[:X.shape[0]]\n",
    "                   for model in tqdm(modelEnsemble.models)]    \n",
    "    temp_array = np.array(predictions)\n",
    "    del(predictions)\n",
    "    temp_array = np.swapaxes(temp_array,0,1)\n",
    "    temp_array = temp_array.reshape(temp_array.shape[0],temp_array.shape[1])\n",
    "    return temp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with h5py.File('tmp_data/ensemble_data.h5', 'r') as hf:\n",
    "#    ensemble_train = hf['ensemble_train'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelEnsemble.models[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble_val = get_ensemble_predictions(X_val, y_val, modelEnsemble)\n",
    "with h5py.File('tmp_data/ensemble_data.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"ensemble_val\",  data=ensemble_val)\n",
    "ensemble_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with h5py.File('tmp_data/ensemble_data.h5', 'r') as hf:\n",
    "#    ensemble_val = hf['ensemble_val'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, X, y , X_test, y_test, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X, label=y)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='logloss', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X, y,eval_metric='logloss')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(X)\n",
    "    dtrain_predprob = alg.predict_proba(X)[:,1]\n",
    "    dtest_predprob = alg.predict_proba(X_test)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators: %d\" % cvresult.shape[0])\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y, dtrain_predictions))\n",
    "    print(\"Log loss (Train): %f\" % metrics.log_loss(y, dtrain_predprob))\n",
    "    print(\"Log loss (Test): %f\" % metrics.log_loss(y_test, dtest_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_train = get_ensemble_predictions(X_train, y_train, modelEnsemble)\n",
    "with h5py.File('tmp_data/ensemble_data.h5', 'a') as hf:\n",
    "    hf.create_dataset(\"ensemble_train\",  data=ensemble_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('tmp_data/ensemble_data.h5', 'r') as hf:\n",
    "    ensemble_train = hf['ensemble_train'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ensemble_all = get_ensemble_predictions(X, y_angle, modelEnsemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=8,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb1, ensemble_val, y_val['is_iceberg'], ensemble_train, y_train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ensemble_all = get_ensemble_predictions(X, y_angle, modelEnsemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':list(range(3,13,2)),\n",
    " 'min_child_weight':list(range(1,10,2))\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=35, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=8, scale_pos_weight=1, seed=random_seed), \n",
    " param_grid = param_test1, scoring='neg_log_loss',n_jobs=1,iid=False, cv=5, verbose=1)\n",
    "gsearch1.fit(ensemble_val,y_val['is_iceberg'].values)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[2.5,3,3.5,6.5,7,7.5]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=35, max_depth=2,\n",
    " min_child_weight=3.5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=8, scale_pos_weight=1, seed=random_seed), \n",
    " param_grid = param_test2, scoring='neg_log_loss',n_jobs=1,iid=False, cv=5, verbose=1)\n",
    "gsearch2.fit(ensemble_val,y_val['is_iceberg'].values)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/20.0 for i in range(0,30)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=35, max_depth=2,\n",
    " min_child_weight=3.5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=8, scale_pos_weight=1, seed=random_seed), \n",
    " param_grid = param_test3, scoring='neg_log_loss',n_jobs=1,iid=False, cv=5, verbose=1)\n",
    "gsearch3.fit(ensemble_val,y_val['is_iceberg'].values)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb2 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=2,\n",
    " min_child_weight=3.5,\n",
    " gamma=1.3,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=8,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb2, ensemble_val, y_val['is_iceberg'],ensemble_train, y_train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=40, max_depth=2,\n",
    " min_child_weight=3.5, gamma=1.3, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=8, scale_pos_weight=1, seed=random_seed), \n",
    " param_grid = param_test4, scoring='neg_log_loss',n_jobs=1,iid=False, cv=5, verbose=1)\n",
    "gsearch4.fit(ensemble_val,y_val['is_iceberg'].values)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(70,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(40,100,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=40, max_depth=2,\n",
    " min_child_weight=3.5, gamma=1.3, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=8, scale_pos_weight=1, seed=random_seed), \n",
    " param_grid = param_test5, scoring='neg_log_loss',n_jobs=1,iid=False, cv=5, verbose=1)\n",
    "gsearch5.fit(ensemble_val,y_val['is_iceberg'].values)\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=40, max_depth=2,\n",
    " min_child_weight=3.5, gamma=1.3, subsample=0.75, colsample_bytree=0.4,\n",
    " objective= 'binary:logistic', nthread=8, scale_pos_weight=1, seed=random_seed), \n",
    " param_grid = param_test6, scoring='neg_log_loss',n_jobs=1,iid=False, cv=5, verbose=1)\n",
    "gsearch6.fit(ensemble_val,y_val['is_iceberg'].values)\n",
    "gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0.000003, 0.00001, 0.0001, 0.0003, 0.0005]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=40, max_depth=2,\n",
    " min_child_weight=3.5, gamma=1.3, subsample=0.75, colsample_bytree=0.4,\n",
    " objective= 'binary:logistic', nthread=8, scale_pos_weight=1, seed=random_seed), \n",
    " param_grid = param_test7, scoring='neg_log_loss',n_jobs=1,iid=False, cv=5, verbose=1)\n",
    "gsearch7.fit(ensemble_val,y_val['is_iceberg'].values)\n",
    "gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb3 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=2,\n",
    " min_child_weight=3.5,\n",
    " gamma=1.3,\n",
    " subsample=0.75,\n",
    " colsample_bytree=0.4,\n",
    " reg_alpha=3e-06,\n",
    " objective= 'binary:logistic',\n",
    " nthread=8,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb3, ensemble_val, y_val['is_iceberg'], ensemble_train, y_train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb4 = xgb.XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=1000,\n",
    " max_depth=2,\n",
    " min_child_weight=3.5,\n",
    " gamma=1.3,\n",
    " subsample=0.75,\n",
    " colsample_bytree=0.4,\n",
    " reg_alpha=3e-06,\n",
    " objective= 'binary:logistic',\n",
    " nthread=8,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb4, ensemble_val, y_val['is_iceberg'], ensemble_train, y_train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb5 = xgb.XGBClassifier(\n",
    " learning_rate =0.03,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=3.5,\n",
    " gamma=0.0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " reg_alpha=1,\n",
    " objective= 'binary:logistic',\n",
    " nthread=8,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb5, ensemble_train, y_train['is_iceberg'], ensemble_val, y_val['is_iceberg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('models/modelXgb4.pickle', 'wb') as handle:\n",
    "    pickle.dump(xgb4, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('models/modelXgb4.pickle', 'rb') as handle:\n",
    "#    xgb4 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use model\n",
    "#model_object_path = \"models\\\\model.CNN_2017_12_19_v15_2017_12_21_15_54_42.hdf5\"\n",
    "#model_best_weights_path = \"models\\\\weights.CNN_2017_12_19_v15_2017_12_21_15_54_42.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_f = load_model(model_object_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_f.load_weights(model_best_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_f.evaluate_generator(datagen_angle_val.flow(X_val, y_val, batch_size=32, shuffle=False), \n",
    "#                          steps = len(X_val)/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_json(\"Data/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df['inc_angle_f'] = pd.to_numeric(test_df['inc_angle'], errors='coerce')\n",
    "print(\"missing values in inc_angle: \", test_df['inc_angle_f'].isnull().sum())\n",
    "test_df['inc_angle_f'].replace(np.nan,0, inplace=True)\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_band_1, t_band_2, t_band_1_t, t_band_2_t, t_band_3, X_test = get_bands(test_df)\n",
    "y_angle_test = test_df.loc[:,['is_iceberg','inc_angle_f']]\n",
    "y_angle_test['index'] = y_angle_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tt = np.append(X_test,X_train, axis=0)\n",
    "X_tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_angle_tt = pd.concat([y_angle_test,y_train])\n",
    "len(y_angle_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(band_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(band_1_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(band_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(band_2_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(band_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(xgb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(xgb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(xgb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(t_band_1,t_band_2,t_band_1_t, t_band_2_t, t_band_3)\n",
    "del(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training denoising model on train and test data - warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lScheduler_denoising = LScheduler(initial_lrate=0.001, drop=0.66, patience=5)\n",
    "lrScheduler_denosing = LearningRateScheduler(lScheduler_denoising.step_decay)\n",
    "#model training\n",
    "start_time = time.monotonic()\n",
    "\n",
    "H = model_denoise.fit_generator(datagen_denoising.flow(X_tt, y_angle_tt, batch_size=8),\n",
    "                    steps_per_epoch=len(X_tt)/8,\n",
    "                    validation_data=datagen_denoising_val.flow(X_tt, y_angle_tt, batch_size=8, shuffle=False), \n",
    "                    validation_steps=len(X_tt)/8,\n",
    "                    #validation_data=[X_val,y_val],\n",
    "                    epochs=10,\n",
    "                    callbacks = [lrScheduler_denosing])\n",
    "\n",
    "model_time = time.monotonic() - start_time\n",
    "print(\"Model training time: \" + '{:d}'.format(int(model_time // 60)) + \" minutes \" \n",
    "      + '{:.1f}'.format(model_time % 60) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model_weights = model_denoise.get_weights()\n",
    "with open('models/model_denoise_weights_tt.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_weights, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble_all = get_ensemble_predictions(X, y_angle, modelEnsemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb4 = xgb.XGBClassifier(\n",
    " learning_rate =0.0325,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=6.5,\n",
    " gamma=0.0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.85,\n",
    " reg_alpha=3e-03,\n",
    " objective= 'binary:logistic',\n",
    " nthread=8,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb4, ensemble_all, y_angle['is_iceberg'], ensemble_train, y_train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(X_tt, y_angle_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(modelEnsemble2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(H,H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(X,y,y_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del(ensemble_val, ensemble_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#idx = 0\n",
    "#for model in modelEnsemble.models:\n",
    "#    pred = get_prediction(model_f, model[2], X_test, y_angle_test)[:X.shape[0]]\n",
    "#    pred = np.array(pred)\n",
    "#    dataset_name = 'ensemble_data_%02d' % idx\n",
    "#    with h5py.File('tmp_data/ensemble_test_data.hd5', 'w') as hf:\n",
    "#        hf.create_dataset(dataset_name,  data=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ensemble_test = get_ensemble_predictions(X_test, y_angle_test, modelEnsemble)\n",
    "#ensemble_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pseudo_labels = xgb4.predict(ensemble_test)\n",
    "#test_probs = xgb4.predict_proba(ensemble_test)\n",
    "#predictions = test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_angle_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_angle_test['is_iceberg'] = pseudo_labels\n",
    "#y_angle_tt = y_angle_test.append(y_train)\n",
    "#y_angle_tt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on pseudo labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lScheduler = LScheduler(initial_lrate=0.00001, drop=0.66, patience=5)\n",
    "#callbacks = [LearningRateScheduler(lScheduler.step_decay)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##model training\n",
    "#start_time = time.monotonic()\n",
    "#\n",
    "#H = model_f.fit_generator(datagen_angle.flow(X_tt, y_angle_tt, batch_size=32),\n",
    "#                    steps_per_epoch=len(X_test)/32,\n",
    "#                    validation_data=datagen_angle_val.flow(X_val, y_val, batch_size=32, shuffle=False), \n",
    "#                    validation_steps=len(X_val)/16,\n",
    "#                    #validation_data=[X_val,y_val],\n",
    "#                    epochs=10, callbacks=callbacks)\n",
    "#\n",
    "#model_time = time.monotonic() - start_time\n",
    "#print(\"Model training time: \" + '{:d}'.format(int(model_time // 60)) + \" minutes \" \n",
    "#      + '{:.1f}'.format(model_time % 60) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions = model_f.predict_generator(datagen_angle_val.flow(X_test, y_angle_test, batch_size=32, shuffle=False), \n",
    "#                           steps = len(X_test)/31, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(predictions[:8424])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#submission = pd.DataFrame({'id': test_df['id'], 'is_iceberg': predictions[:8424].reshape(-1)})\n",
    "#submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#submission.to_csv(\"submission.v24.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
